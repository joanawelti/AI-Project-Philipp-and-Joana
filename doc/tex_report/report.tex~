\documentclass[a4paper,10pt]{scrartcl}
\usepackage[UKenglish]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{listings}
%\usepackage{eso-pic}

%opening
\title{Report for Assessment 1 \\ \large{of CS3616}}
\author{Joana Welti, Philipp Lucas}

\begin{document}

% A PDF of your report which describes the learning techniques you have implemented and shows graphs of their performance (the filename is your_name.pdf). It should also give me instructions on what I need to do to play your game and see all the things you have implemented. It is important that you remember to put important features of your program in your report, because if you have special things implemented that are not described in your report, then I might not notice them and you won't get credit for them.

\maketitle
\newpage

\tableofcontents

\newpage

\section{Scenario}
In our scenario there is one fast ogre (let's call him John) in fraction ``blue`` and a variable number of slower creatures in fraction ''yellow``. The primary goal of the fast ogre is to escape from the dungeon by reaching the exit. For this it is not necessary to kill all the enemies however it might become necessary to fight against some of them as they block the way out.\par
The normal weapon used by John is the longbow, as this allows him to attack without being attacked himself. In addition he can decide to pickup use potions to regain health or energy.

\section{Reinforcement Learning}
The learning techniques we decided to employ is reinforcement learning. 

\subsection{States}
We use the following state variables, all of which are devided into 5 equidistant intervals:
\begin{itemize}
 \item health points of Tom
 \item energy points of Tom
 \item distance between Tom and closest enemy
\end{itemize}

\subsection{Actions}
Tom can take the following actions:
\begin{itemize}
 \item attack closest enemy
 \item evade close enemies
 \item escape to exit
 \item get and use closest health potion
 \item get and use closest energy potion
\end{itemize}

''closest`` in this context means absolut distance and ''not distance to walk``. \par

''evade close enemies`` lets Tom move directly away from the geometric center of all enemies that are within a certain radius from Tom.

\subsection{Implementation}
We implemented the reinforcement learning in a rather generic way. \par

\paragraph{Qtable.java}
This class holds a Q-table to be used in reinforcement learning. As long as Action.java and State.java provide all necessary methods Q-Table is entirely independent on the actual actions or states. It provides a number of methods to update the qtable and retrieve actions: \par

\textit{static void updateTable (double reward, State oldState, State newState, Action oldAction)} updates the Q-table using the given parameters, i.e. in our case the previous state John was in, the current state John is in, the action that led to this state and the reward that is to be assigned for this. \par

\textit{static Action getRandomAction()} returns a random action, i.e. on of those defined in Action.java. \par

\textit{static Action getGreedyAction(State state)} returns to  greediness \% the best, otherwise a random action. \par

\paragraph{Action.java}
Action.java is a enumeration of all actions that can be taken. See the source code for a description of the methods. \par


To add a new action, add the name of that action to the enumeration and assign a reward for this action in \textit{double getReward()}. Also add the actual action in your \textit{doAction(Action action)} method in your behaviour class.

\paragraph{State.java}
State.java handles the different states needed for reinforcement learning. It provides a number of public methods: \par

\textit{boolean hasNotChanged(State state)} Returns true of this state and state share the same index. \par

\textit{int getMaxIndex()} Returns the number of possible different indices. \par

\textit{int getIndex()} Returns the index of this state, which is a number between (including) 0 and (excluding) getMaxIndex(). \par


\subsection{Graphs}


\section{How to play}

\end{document}