\documentclass[a4paper,10pt]{scrartcl}
\usepackage[UKenglish]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{listings}
%\usepackage{eso-pic}

%opening
\title{Report for Assessment 1 \\ \large{of CS3616}}
\author{Joana Welti, Philipp Lucas}

\begin{document}

% A PDF of your report which describes the learning techniques you have implemented and shows graphs of their performance (the filename is your_name.pdf). It should also give me instructions on what I need to do to play your game and see all the things you have implemented. It is important that you remember to put important features of your program in your report, because if you have special things implemented that are not described in your report, then I might not notice them and you won't get credit for them.

\maketitle
\newpage

\tableofcontents

\newpage

\section{Scenario}
In our scenario there is one fast ogre (let's call him John) in fraction ``blue`` and a variable number of slower creatures in fraction ''yellow``. The primary goal of the fast ogre is to escape from the dungeon by reaching the exit. For this it is not necessary to kill all the enemies however it might become necessary to fight against some of them as they block the way out.\par
The normal weapon used by John is the longbow, as this allows him to attack without being attacked himself. In addition he can decide to pickup use potions to regain health or energy.

\section{Reinforcement Learning}
The learning techniques we decided to employ is reinforcement learning. 

\subsection{States}
We use the following state variables, all of which are divided into 5 equidistant intervals:
\begin{itemize}
 \item health points of Tom
 \item energy points of Tom
 \item distance between Tom and closest enemy
\end{itemize}

\subsection{Actions}
Tom can take the following actions:
\begin{itemize}
 \item attack closest enemy
 \item evade close enemies
 \item escape to exit
 \item get and use closest health potion
 \item get and use closest energy potion
\end{itemize}

''closest`` in this context means absolute distance and ''not distance to walk``. \par

''evade close enemies`` lets Tom move directly away from the geometric center of all enemies that are within a certain radius from Tom.

\subsection{Rewards}
Tm gets the following rewards or penalties
\begin{center}
\begin{tabular}{l|l}
Action & Reward \\
\hline
reached exit & +10 \\
died & -10 \\
hit enemy & +3 \\
got hit by enemy & -3 \\
kill enemy & +5 \\
used potion & depends on current health/energy \\
changed state & 0
\end{tabular}
\end{center}

''used potion``: rewards depends on how much energy/health Tom regained doing this. TODO: explain in detail \par
''changed state``: if Tom changed state but didn't achieve anything of the above.

\subsection{Implementation}
We implemented the reinforcement learning in a rather generic way. \par

\paragraph{Qtable.java}
This class holds a Q-table to be used in reinforcement learning. As long as Action.java and State.java provide all necessary methods Q-Table is entirely independent on the actual actions or states. It provides a number of methods to update the qtable and retrieve actions: \par
\begin{itemize}
 \item \textit{static void updateTable (double reward, State oldState, State newState, Action oldAction)} updates the Q-table using the given parameters, i.e. in our case the previous state John was in, the current state John is in, the action that led to this state and the reward that is to be assigned for this.
 \item \textit{static Action getRandomAction()} returns a random action, i.e. on of those defined in Action.java.
 \item \textit{static Action getGreedyAction(State state)} returns to  greediness \% the best, otherwise a random action.
\end{itemize}

\paragraph{Action.java}
Action.java is a enumeration of all actions that can be taken. See the source code for a description of the methods. \par

To add a new action, add the name of that action to the enumeration and assign a reward for this action in \textit{double getReward()}. Also add the actual action in your \textit{doAction(Action action)} method in your behaviour class.

\paragraph{State.java}
State.java handles the different states needed for reinforcement learning. It provides a number of public methods:
\begin{itemize}
 \item \textit{boolean hasNotChanged(State state)} Returns true of this state and state share the same index.
 \item \textit{int getMaxIndex()} Returns the number of possible different indices.
 \item \textit{int getIndex()} Returns the index of this state, which is a number between (including) 0 and (excluding) getMaxIndex().
\end{itemize} 

To change the states you only need to adapt the following:
\begin{itemize}
 \item update getMaxIndex() accordingly.
 \item update setIndex() accordingly.
\end{itemize}

(currently the class has much more private variables. However, in fact there is no need (exept for debugging) to store them as only the index matters: reading them from the game, then calculating the index and forgetting about all state variables after this again would be perfectly fine.)

\subsection{Graphs}


\section{How to run the scenario}

To run the scenario just run the game and load \textit{findExit.xml}. 




\end{document}