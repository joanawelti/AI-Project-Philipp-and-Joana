\documentclass[a4paper,10pt]{scrartcl}
\usepackage[UKenglish]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{listings}
%\usepackage{eso-pic}

%opening
\title{Report for Assessment 1 \\ \large{of CS3616}}
\author{Joana Welti, Philipp Lucas}
\date{April, 2011}

\begin{document}

% A PDF of your report which describes the learning techniques you have implemented and shows graphs of their performance (the filename is your_name.pdf). It should also give me instructions on what I need to do to play your game and see all the things you have implemented. It is important that you remember to put important features of your program in your report, because if you have special things implemented that are not described in your report, then I might not notice them and you won't get credit for them.

\maketitle
\newpage

\tableofcontents

\newpage


\section{Introduction}
This paper describes the project implemented for the first assessment of the Artificial Intelligence For Computer Games course. 

Our project is based on reinforced learning, a machine learning technique, which we used to implement the artificial intelligence for the dungeon game used in the lecture. 

Our main achievements are the implementation of a generic reinforcement learner which allows to separate the learning process from the specific scenario and the design of a flexible configuration functionality, which enables users to pass parameters to the dungeon. 

The project includes a new scenario, which is described in section~\ref{sec:scenario}. Sections~\ref{sec:implementation} and \ref{sec:rl} deal with the implementation and section\ref{sec:instructions} explains how to run our scenario. 
 
\section{Scenario}\label{sec:scenario}
In our scenario there is one fast ogre (let's call him John) in fraction ``blue`` and a variable number of slower creatures in fraction ''yellow``. The primary goal of John is to escape from the dungeon by reaching the exit. For this it is not necessary to kill all the enemies however it might become necessary to fight against some of them as they block the way out. 

The normal weapon used by John is the longbow, as this allows him to attack without being attacked himself. In addition he can decide to pickup and use potions to regain health or energy.

\section{Reinforcement Learning}\label{sec:rl}
The learning techniques we decided to employ is reinforcement learning. Reinforced learning allows actors to learn what actions to take when being in a certain state based on rewards or penalties they got earlier on.

We decided to use reinforcement learning because we provide different actions to take and John should learn which of them is appropriate for a specific action. Reinforced learning achieves exactly this behaviour with the help of the rewards/penalties, without having the user to play and train the game first. John is also able to find out the goal of the game by himself in a completely automated way with this technique. 

\subsection{States}
We use the following state variables:
\begin{itemize}
 \item health points of John - divided in 3 equidistant intervals.
 \item energy points of John - divided in 3 equidistant intervals.
 \item distance between John and closest enemy  - divided in 5 equidistant intervals.
 \item distance between John and the exit - divided in 5 equidistant intervals.
\end{itemize}

\subsection{Actions}
John can take the following actions:
\begin{itemize}
 \item attack closest enemy
 \item evade close enemies
 \item escape to exit
 \item get and use closest health potion
 \item get and use closest energy potion
\end{itemize}

''closest`` in this context means absolute distance and ''not distance to walk``.

''evade close enemies`` lets John move directly away from the geometric center of all enemies that are within a certain radius from John.

\subsection{Rewards}
Tm gets the following rewards or penalties
\begin{center}
\begin{tabular}{l|l}
Action & Reward \\
\hline
reached exit & +10 \\
died & -10 \\
hit enemy & +3 \\
got hit by enemy & -3 \\
kill enemy & +5 \\
used potion & variable\\
changed state & 0
\end{tabular}
\end{center}

\minisec{''reached exit`` and ``died''Rewards}
Reaching the exit and dying are the two ends of the spectrum, so John gets the biggest reward/penalty respectively.

\minisec{''hit enemy`` and ``got hit by enemy'' Rewards}
Killing all the monsters is not the main objective, so John only gets a small reward for hitting any enemies.

\minisec{''used potion`` Rewards}
The reward John gets for picking up an health potion depends on his current health. He gets the most points when his health level is low. As getting to the exit is the goal of the game, he gets a slight penalty when he picks up a potion although he's health level is good and he wouldn't need one. 
For health levels in between he gets a gradual amount of reward. 
With this reward system, we want to train John to pick up health potions when he needs them in order to win the game.
 
The same rules apply also for energy potions. 

\minisec{''changed state`` Rewards}
When John changes state but doesn't achieve anything listed above, he doesn't get any reward or penalty. 


\section{Implementation}\label{sec:implementation}
\subsection{Reinforcement Learner}
% We implemented the reinforcement learning in a rather generic way.
% % This is our main achievement! TODO: rephrase?
Our implementation allows to separate the Q-Learner completely from the specific scenario. It is up to the client to define what attribute a state constitutes and what actions an actor can take, which makes our Q-Learner reusable and generic. 

\paragraph{Qtable.java}
This class holds a Q-table to be used in reinforcement learning. As long as \verb|Action.java| and \verb|State.java| provide all necessary methods Q-Table is entirely independent on the actual actions or states. It provides a number of methods to update the qtable and retrieve actions:

\begin{itemize}
 \item \verb|static void updateTable (double reward, State oldState,|\\
  \verb|State newState, Action oldAction)| updates the Q-table using the given parameters, i.e. in our case the previous state John was in, the current state John is in, the action that led to this state and the reward that is to be assigned for this.
 \item \verb|static Action getRandomAction()| returns a random action, i.e. on of those defined in Action.java.
 \item \verb|static Action getGreedyAction(State state)| returns to  greediness \% the best, otherwise a random action.
\end{itemize}

\paragraph{Action.java}
\verb|Action.java| is a enumeration of all actions that can be taken. See the source code for a description of the methods.

To add a new action, add the name of that action to the enumeration and assign a reward for this action in \verb|double getReward()|. Also add the actual action in your \verb|doAction(Action action)| method in your behaviour class.

\paragraph{State.java}
\verb|State.java| handles the different states needed for reinforcement learning. It provides a number of public methods:
\begin{itemize}
 \item \verb|boolean hasNotChanged(State state)| Returns true of this state and state share the same index.
 \item \verb|int getMaxIndex()| Returns the number of possible different indices.
 \item \verb|int getIndex()| Returns the index of this state, which is a number between (including) 0 and (excluding) \verb|getMaxIndex()|.
\end{itemize} 

To change the states, you only need to adapt the following:
\begin{itemize}
 \item update \verb|getMaxIndex()| accordingly.
 \item update \verb|setIndex()| accordingly.
\end{itemize}

(currently the class has much more private variables. However, in fact there is no need (except for debugging) to store them as only the index matters: reading them from the game, then calculating the index and forgetting about all state variables after this again would be perfectly fine.)

\subsubsection{Visualisation of the learning}
To visualize how the behaviour of the John improves, the percentage of the games won out of the last 50 rounds is displayed.


\subsubsection{Graphs}
% TODO: what is x/y axis?

\begin{figure}[htp]
\centering
%\includegraphics{}
\caption{}\label{fig:randomness_90_ogres_2}
\end{figure}

\begin{figure}[htp]
\centering
%\includegraphics{}
\caption{}\label{fig:randomness_99_ogres_2}
\end{figure}


\begin{figure}[htp]
\centering
%\includegraphics{}
\caption{}\label{fig:randomness_90_ogres_3}
\end{figure}

\begin{figure}[htp]
\centering
%\includegraphics{}
\caption{}\label{fig:randomness_99_3}
\end{figure}


\minisec{Setup}
Common configurations for all figures: 
\begin{center}
\begin{tabular}{|l|l|}
\hline
\textbf{Initialization values for Q-Table} & 0.01 \\
\hline
\textbf{Alpha values} & 0.5 \\
\hline
\textbf{Discount factor} & 0.9 \\
\hline
\end{tabular}
\end{center}

Figure~\ref{fig:randomness_90_ogres_2}:
\begin{center}
\begin{tabular}{|l|l|}
\hline
\textbf{Number of Ogres} & 2 \\
\hline
\textbf{Greediness} & 0.9 \\
\hline
\end{tabular}
\end{center}

Figure~\ref{fig:randomness_90_ogres_3}:
\begin{center}
\begin{tabular}{|l|l|}
\hline
\textbf{Number of Ogres} & 3 \\
\hline
\textbf{Greediness} & 0.9 \\
\hline
\end{tabular}
\end{center}

Figure~\ref{fig:randomness_99_ogre_2}:
\begin{center}
\begin{tabular}{|l|l|}
\hline
\textbf{Number of Ogres} & 2 \\
\hline
\textbf{Greediness} & 0.99 \\
\hline
\hline
\end{tabular}
\end{center}

Figure~\ref{fig:randomness_99_ogre_3}:
\begin{center}
\begin{tabular}{|l|l|}
\hline
\textbf{Number of Ogres} & 3 \\
\hline
\textbf{Greediness} & 0.99 \\
\hline
\end{tabular}
\end{center}

\minisec{Interpretation}
% TODO: write something about the included graphs. What do the graphs say?

Clearly, randomness is not only a blessing in our scenario. We need some randomness to make John explore the world and to actually find out what his goal is, but once he's won a couple of times, he shouldn't take too many random actions. Because we have quite a few actions and also quite a few enemy ogres for this small world, one wrong action can mean the death of John. 

% TODO: is this correct?
Figure~\ref{fig:randomness_99_ogre_2} and \ref{fig:randomness_99_ogre_3} show that John performs much better with a higher greediness factor, which means that he chooses less random actions.

Unfortunately, John doesn't win every time, even after having trained for 2000 games. We would have to investigate this issue further to draw a valid conclusion from this fact. 

\subsection{Configuration functionality}
Our configuration functionality makes it possible for a scenario to have configuration options. The GUI provides a settings panel where the adjustable parameters are displayed and can be changed by the user during run time.

\subsubsection{Usage}
A scenario XML can have configuration options for at most one behaviour. In order to specify which settings should be loaded for a particular scenario, XMLs can include a \verb|Configurations| element. The \verb|type| attribute refers to the name of the configuration class that is to be used:
\begin{verbatim}
<Configurations Type="ReinforcementLearnerConfigurations"/>
\end{verbatim}
In this example, the dungeon GUI would display configurations as specified in the  \verb|ReinforcementLearnerConfigurations| class. These configuration classes have to be located in the \verb|dungeon.configurations| package.


\subsubsection{Implementation}
\minisec{dungeon.configurations}
Every behaviour that is customizable needs to have a class in this package that implements the abstract \verb|Configurations| class.
\begin{itemize}
\item \verb|Configurations|: Each parameter in a configuration has a key. The parameters of a configuration can be retrieved as a \verb|HashMap|, where the key is used to access parameter values such as the default value, current value, min/max value possible etc. 
\item \verb|ConfigurationHandler|: Loads configuration class as specified in the XML (type attribute of \verb|Configurations| element) with the help of Java reflection.
\item \verb|ReinforcementLearnerConfigurations|: specific configurations for the reinforcement learning problem (initialization values for Q-Table, values to be used for Alpha/Discountfactor/Greediness)
\end{itemize}

\minisec{dungeon.ui}
\begin{itemize}
\item \verb|ParameterPanel|: GUI element for the settings menu. Lists the configuration options of the specific \verb|Configuration| class which is used in the game. Independent of the kind of settings used.
\item \verb|DungeonForm|: passes  the configuration entered by the user to the game class every time the game is restarted. Displays the configuration option as spinners if the current scenario has any.
\end{itemize}


\section{How to run the scenario}\label{sec:instructions}
To run the scenario, start the game and load the \verb|findExit.xml| file. \verb|findExit.xml| can be found in the \verb|xml| folder.  Change any configuration values if needed on the settings pannel and click on start.  




\end{document}